{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview: We first define all the necessary functions before executing the code in the following order: 1) Load the semantic space 2) Load the data 3) Determine the target word's neighbours 4) Compute the different values of OSC and PSC 5) Check correlation of OSC and PSC with AoA 6) Load all the variables into a final dataframe.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot \n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "from old20 import old20\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import semspaces\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from jellyfish import levenshtein_distance\n",
    "import sklearn\n",
    "import sklearn.metrics.pairwise as smp\n",
    "from celex.utilities.dictionaries import tokens2ids\n",
    "from collections import defaultdict\n",
    "from semspaces.space import SemanticSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aoa(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:        the path to the file AoA_ratings_from_all_sources.xlsx from the Age of Acquisition norms\n",
    "                        collected by Kuperman et al 2012. The exact file can be downloaded here:\n",
    "                        http://crr.ugent.be/papers/AoA_ratings_from_all_sources.zip\n",
    "    :return:            the set of words for which age of acquisition norms were collected.\n",
    "    \"\"\"\n",
    "\n",
    "    aoa_norms = pd.read_excel(path, usecols = [\"Word\",\"Rating.Mean\"])\n",
    "    aoa_words = set(aoa_norms['Word'])\n",
    "\n",
    "    return aoa_words, aoa_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Frequency (SUBTLEX-US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frequency_subtlex(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:            the path to the csv file containing the SUBTLEX-US frequency values \n",
    "    :return:                a dict mapping words to frequency values\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    df[\"word\"] = df[\"Word\"].str.lower()\n",
    "    word2freq = pd.Series(df['SUBTLWF'].values, index=df['word']).to_dict()\n",
    "\n",
    "    return word2freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concreteness_norms(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:            the path to the .txt file containing the concreteness norms from Brysbaert et al.\n",
    "    :return:                a dict mapping words to concreteness scores\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, sep='\\t', header=0)\n",
    "    df[\"word\"] = df[\"Word\"].str.lower()\n",
    "    word2concr = pd.Series(df['Conc.M'].values, index=df['word']).to_dict()\n",
    "\n",
    "    return word2concr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_valence_norms(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:            the path to the csv file containing the valence values from Brysbaert et al.\n",
    "    :return:                a dict mapping words to valence scores\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    df[\"word\"] = df[\"Word\"].str.lower()\n",
    "    word2val = pd.Series(df['V.Mean.Sum'].values, index=df['word']).to_dict()\n",
    "\n",
    "    return word2val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_old20(path):\n",
    "    \"\"\"\n",
    "    :param path:            the path to the csv file containing the OLD20 values\n",
    "    :return:                a dict mapping words to valence scores\n",
    "    \n",
    "    If OLD20 csv is missing, use the function compute_old20() which can be found below. \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, sep=' ', header=None)\n",
    "    word2old = pd.Series(df[1].values, index=df[0]).to_dict()\n",
    "    return word2old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iconicity(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:            the path to the csv file containing the SUBTLEX-US frequency values \n",
    "    :return:                a dict mapping words to frequency values\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    word2iconicity = pd.Series(df['Iconicity'].values, index=df['Word']).to_dict()\n",
    "\n",
    "    return word2iconicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection & Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_words(*args):\n",
    "\n",
    "    \"\"\"\n",
    "    :param args:    an arbitrary number of sets (other structures are coerced to set)\n",
    "    :return:        the intersection between the input arguments\n",
    "    \"\"\"\n",
    "\n",
    "    shared = set()\n",
    "    for i, arg in enumerate(args):\n",
    "        if arg:\n",
    "            if not isinstance(arg, set):\n",
    "                print(\"Input argument {}, originally of type {}, is coerced to set\".format(i, type(arg)))\n",
    "                arg = set(arg)\n",
    "            if not shared:\n",
    "                shared = arg\n",
    "            else:\n",
    "                shared = shared.intersection(arg)\n",
    "\n",
    "    return shared\n",
    "\n",
    "\n",
    "def union_words(*args):\n",
    "\n",
    "    \"\"\"\n",
    "    :param args:    an arbitrary number of sets (other structures are coerced to set)\n",
    "    :return:        the union of the input arguments\n",
    "    \"\"\"\n",
    "\n",
    "    union_set = set()\n",
    "    for i, arg in enumerate(args):\n",
    "        if arg:\n",
    "            if not isinstance(arg, set):\n",
    "                print(\"Input argument {}, originally of type {}, is coerced to set\".format(i, type(arg)))\n",
    "                arg = set(arg)\n",
    "            if not union_set:\n",
    "                union_set = arg\n",
    "            else:\n",
    "                union_set = union_set.union(arg)\n",
    "\n",
    "    return union_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mono(path):\n",
    "\n",
    "    \"\"\"\n",
    "    :param path:    str, indicating the path to the morpholex database\n",
    "    :return:        a set of lower-cased strings indicating the mono-morphemic words found in the morpholex database\n",
    "    \"\"\"\n",
    "\n",
    "    morpholex_df = pd.read_excel(path, sheet_name=1)\n",
    "    w = set(morpholex_df['MorphoLexSegm'])\n",
    "    targets = set()\n",
    "    regex = re.compile('[^a-z]')\n",
    "    for el in w:\n",
    "        try:\n",
    "            targets.add(regex.sub('', el.lower()))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def read_mono_inflected(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param path:    str, indicating the path to the morpholex database\n",
    "    :return:        a set of lower-cased strings indicating the mono-morphemic words with inflectional morphemes\n",
    "                    found in the morpholex database\n",
    "    \"\"\"\n",
    "\n",
    "    targets = set()\n",
    "    inflections = ['s', 'ed', 'ing', 'en', \"'s\", 'er', 'est', 'es', 'ies', 'ings', 'ied']\n",
    "    morpholex = pd.read_excel(path, sheet_name=None)\n",
    "    for name, sheet in morpholex.items():\n",
    "        if name == '0-1-0':\n",
    "            token2base = pd.Series(sheet['MorphoLexSegm'].values, index=sheet['Word']).to_dict()\n",
    "            for token, base in token2base.items():\n",
    "                if type(token) == int or type(token) == float:\n",
    "                    continue\n",
    "                base = base.strip('{{()}}')\n",
    "                token = token.lower()\n",
    "                if token != base:\n",
    "                    inflected = [''.join([base, affix]) for affix in inflections]\n",
    "                    reduplicated_final = ''.join([token, token[-1]])\n",
    "                    inflected.extend([''.join([reduplicated_final, affix]) for affix in inflections])\n",
    "                    if base[-1] == 'f' or token[-2:] == 'fe':\n",
    "                        f_v_alternation = re.sub('(f|fe)$', 'v', base)\n",
    "                        inflected.extend([''.join([f_v_alternation, affix]) for affix in inflections])\n",
    "                    if base[-1] == 'e':\n",
    "                        no_e = re.sub('e$', '', base)\n",
    "                        inflected.extend([''.join([no_e, affix]) for affix in inflections])\n",
    "                    if base[-1] == 'y':\n",
    "                        no_y = re.sub('y$', '', base)\n",
    "                        inflected.extend([''.join([no_y, affix]) for affix in inflections])\n",
    "                    if token == 'vertices':\n",
    "                        inflected.append(token)\n",
    "                    if base.endswith('c'):\n",
    "                        plus_k = base + 'k'\n",
    "                        inflected.extend([''.join([plus_k, affix]) for affix in inflections])\n",
    "                    if base[-1] in {'b', 'd', 'f', 'g', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'z'}:\n",
    "                        reduplicated = base + base[-1]\n",
    "                        inflected.extend([''.join([reduplicated, affix]) for affix in inflections])\n",
    "                    if base.endswith('eau'):\n",
    "                        inflected.extend([''.join([base, affix]) for affix in ['s', 'x']])\n",
    "\n",
    "                    o_to_ou = re.sub('o[bcdfgklmnpqrstvxz]+$', 'ou', base)\n",
    "                    inflected.extend([''.join([o_to_ou, affix]) for affix in inflections])\n",
    "                    if token in inflected:\n",
    "                        targets.add(token)\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def read_poly(path):\n",
    "    \"\"\"\n",
    "    :param path:    str, indicating the path to the morpholex database\n",
    "    :return:        a set of lower-cased strings indicating the poly-morphemic words found in the morpholex database\n",
    "    \"\"\"\n",
    "\n",
    "    targets = set()\n",
    "    regex = re.compile('[^a-z]')\n",
    "    morpholex = pd.ExcelFile(path)\n",
    "    for idx, name in enumerate(morpholex.sheet_names):\n",
    "        try:\n",
    "            a, b, c = name.split('-')\n",
    "            if a != '0' or c != '0':\n",
    "                sheet = morpholex.parse(name)\n",
    "                w = set(sheet['MorphoLexSegm'])\n",
    "                for el in w:\n",
    "                    try:\n",
    "                        targets.add(regex.sub('', el.lower()))\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celex_coverage(words, celex_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    :param words:       a set of target words to be encoded phonologically\n",
    "    :param celex_dict:  the dictionary derived from the CELEX database\n",
    "    :return:    - a set containing the words among the targets for which a unique phonological transcription was found\n",
    "                - a set containing the words among the targets for which more than one phonological transcription was\n",
    "                    found\n",
    "                - a set containing the words among the targets for which no phonological transcription was found\n",
    "    \"\"\"\n",
    "\n",
    "    tokens2identifiers = tokens2ids(celex_dict)\n",
    "\n",
    "    no_phon = set()\n",
    "    ambiguous = set()\n",
    "    spoken_words_phonology = set()\n",
    "\n",
    "    for word in words:\n",
    "        token_ids = tokens2identifiers[word]\n",
    "        possible_phonological_transcriptions = set()\n",
    "        if token_ids:\n",
    "            for token_id in token_ids:\n",
    "                possible_phonological_transcriptions.add(celex_dict['tokens'][token_id]['phon'])\n",
    "            if len(possible_phonological_transcriptions) > 1:\n",
    "                ambiguous.add((word, tuple(possible_phonological_transcriptions)))\n",
    "            else:\n",
    "                for transcription in possible_phonological_transcriptions:\n",
    "                    spoken_words_phonology.add((word, transcription.replace(\"-\", \"\")))\n",
    "        else:\n",
    "            no_phon.add(word)\n",
    "\n",
    "    return spoken_words_phonology, ambiguous, no_phon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create morphological complexity variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_morph_complexity(shared_words, mono_words, mono_inflected_words, poly_words):\n",
    "    \"\"\"\n",
    "    :param shared_words:    set (or list), the shared words of all data sets \n",
    "    :mono_words:            list, monomorphemic words extracted from MorphoLEX\n",
    "    :mono_inflected_words:  list, monomorphemic words with inflections extracted from MorphoLEX\n",
    "    :poly_words:            list, polymorphmeic words extracted from MorphoLEX\n",
    "    :return:                dictionary, words mapped to morphological status (monomorphemic = 0; polymorphemic = 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    dd = defaultdict(int)\n",
    "    for word in shared_words:\n",
    "        if word in mono_words or word in mono_inflected_words:\n",
    "            dd[word] = 0\n",
    "        elif word in poly_words:\n",
    "            dd[word] = 1\n",
    "        else:\n",
    "            continue\n",
    "    return dd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute semantic neighbourhood density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snd(sem_space, target_words, filter_words, n):\n",
    "    \"\"\"\n",
    "    :sem_space:      the semantic space\n",
    "    :target_words:   list, target words\n",
    "    :filter_words:   list, overlap of word lists of other control variables (see filter_words below)\n",
    "    :n:              int, number of neighbours to consider\n",
    "    :return:         dictionary, SND value, higher values refer to sparser semantic neighbourhoods)\n",
    "    \"\"\"\n",
    "    d = defaultdict(float)\n",
    "    for word in tqdm(target_words):\n",
    "        distances_df = sem_space.all_distances([word])\n",
    "        filtered_distances_df = distances_df.loc[:, distances_df.columns.isin(filter_words)]\n",
    "        sorted_distances = filtered_distances_df.loc[word].sort_values(ascending = True)\n",
    "        d[word] = sum(sorted_distances[1:n+2])/n #index 0 is identical to target word, so skip it\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to determine neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbours defined as target-embedded words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_TE_neighbours(targets, reference_words):\n",
    "    \"\"\"\n",
    "    :param targets:         list, target words OR dictionary, target words mapped to phonological represenation\n",
    "    :param reference_words: list, reference words \n",
    "    :return:                dictionary, target words mapped to neighbours (target-embedded words)\n",
    "    \"\"\"\n",
    "    dn = defaultdict(list)\n",
    "    \n",
    "    if type(targets) == dict:\n",
    "        #create phonological representation of reference words\n",
    "        ref_phon = {k: v for k, v in get_celex_coverage(reference_words, celex)[0]}\n",
    "        for target, phon_t in tqdm(targets.items()):\n",
    "            for ref, phon_r in ref_phon.items():\n",
    "                if phon_t in phon_r and phon_t != phon_r and phon_r not in dn[target]:\n",
    "                    dn[target].append(ref)\n",
    "            if dn[target] == []:\n",
    "                dn[target] = random.sample(reference_words, 15) #19 - average number of neighbours in original set of target words \n",
    "            \n",
    "    else:\n",
    "        for word in tqdm(targets):\n",
    "            for ref in reference_words:\n",
    "                if word in ref and word != ref and ref not in dn[word]:\n",
    "                    dn[word].append(ref)\n",
    "            if dn[word] == []:\n",
    "                dn[word] = random.sample(reference_words, 10) # 13 - average number of neighbours in original set of target words \n",
    "\n",
    "    return dn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbours defined by Levenshtein edit-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LD_neighbours(targets, reference_words, d=None, rn=None, kn=None):\n",
    "    \"\"\"\n",
    "    :param targets:         list, target words OR dictionary, target words mapped to phonological representation\n",
    "    :param reference_words: list, reference words \n",
    "    :param d:               int, maximum edit distance to consider\n",
    "    :param rn:              int, number of random neighbours to consider if none are found within distance d\n",
    "    :param kn:              int, number of neighbours to consider when d is not predefined\n",
    "    :return:                dictionary, target words mapped to neighbours with edit distance <= d\n",
    "    \"\"\"\n",
    "    dd = defaultdict(dict)\n",
    "    dn = defaultdict(list)\n",
    "    \n",
    "    if type(targets) == dict:\n",
    "        #create phonological representation of reference words\n",
    "        ref_phon = {k: v for k, v in get_celex_coverage(reference_words, celex)[0]}\n",
    "        for target, phon_t in tqdm(targets.items()):\n",
    "            for ref, phon_r in ref_phon.items():\n",
    "                dd[target][ref] = levenshtein_distance(phon_t, phon_r)\n",
    "            values = [v for w, v in sorted(dd[target].items(), key=lambda item: item[1])]            \n",
    "            d = values[kn] #not kn-1 because first element in list is identical to target\n",
    "            \n",
    "            for word, dis in dd[target].items():\n",
    "                if dis <= d and dis > 0:\n",
    "                    dn[target].append((word, dis))\n",
    "            #if dn[target] == []:\n",
    "                #dn[target] = random.sample(reference_words, rn)\n",
    "\n",
    "    else:\n",
    "        #find neighbours for all words in reference_words \n",
    "        for target in tqdm(targets):\n",
    "            for ref in reference_words:\n",
    "                dd[target][ref] = levenshtein_distance(target, ref)\n",
    "            values = [v for w, v in sorted(dd[target].items(), key=lambda item: item[1])]\n",
    "            d = values[kn]\n",
    "                \n",
    "            for word, dis in dd[target].items():\n",
    "                if dis <= d and dis > 0:\n",
    "                    dn[target].append((word, dis))\n",
    "            #if dn[target] == []:\n",
    "                #dn[target] = random.sample(dd[target].items(), rn) \n",
    "                \n",
    "\n",
    "    return dn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to compute OSC & PSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSC & PSC - Target embedded neighbours & frequency normalized (Marelli & Amenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_OPSC_ma(tn_dict):\n",
    "    \"\"\"\n",
    "    :param tn_dict:    dictionary, target words mapped to list of target-embedded neighbours \n",
    "    :return:           dictionary, target words mapped to O/PSC value\n",
    "    \"\"\"\n",
    "    opsc_dict = defaultdict(float)\n",
    "\n",
    "    for target in tqdm(tn_dict):\n",
    "        nom = 0\n",
    "        denom = 0\n",
    "        target_vector = space.get_vector(target)\n",
    "        \n",
    "        for n in set(tn_dict[target]):\n",
    "            denom += freq_subtlex[n]\n",
    "            \n",
    "            try:\n",
    "                neighbour_vector = space.get_vector(n)\n",
    "                    \n",
    "                sim = abs(sklearn.metrics.pairwise.cosine_similarity(target_vector, neighbour_vector))\n",
    "                nom += sim * freq_subtlex[n]\n",
    "                \n",
    "            except ValueError:\n",
    "                continue  \n",
    "                \n",
    "        opsc_dict[target] = float(nom/denom)\n",
    "    \n",
    "    return opsc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSC & PSC - edit-distance & nr. of neighbours normalized (Hendrix & Sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_OPSC_hs(tn_dict):\n",
    "    \"\"\"\n",
    "    :param tn_dict:    dictionary, target words mapped to list of (neighbour, distance) tuples\n",
    "    :return:           dictionary, target words mapped to O/PSC value\n",
    "    \"\"\"\n",
    "    opsc_dict = defaultdict(int)\n",
    "    \n",
    "    for target in tqdm(tn_dict):\n",
    "        if target == \"the\":\n",
    "            continue\n",
    "        nom = 0\n",
    "        denom = len(tn_dict[target])\n",
    "        target_vector = space.get_vector(target)\n",
    "        \n",
    "        for n, ed_dis in tn_dict[target]:\n",
    "            try:\n",
    "                neighbour_vector = space.get_vector(n)\n",
    "                    \n",
    "                sim = abs(sklearn.metrics.pairwise.cosine_similarity(target_vector, neighbour_vector))\n",
    "                nom += sim * 1/ed_dis\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        opsc_dict[target] = float(nom/denom)\n",
    "    \n",
    "    return opsc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to check correlation with AoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correlation(targets, d, aoa_dict):\n",
    "    \"\"\"\n",
    "    :param targets:   list, target words\n",
    "    :param targets:   dictionary, target words mapped to OSC or PSC values\n",
    "    :aoa_dict:        dictionary, target words mapped to AoA ratings\n",
    "    :return:          float, pearson correlation between O/PSC and AoA \n",
    "    :return:          scatter, scatterpolot of correlation between O/PSC and AoA\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    aoa = []\n",
    "    for target in targets:\n",
    "        values.append(d[target])\n",
    "        aoa.append(aoa_dict[target])\n",
    "    scatter = pyplot.scatter(values,aoa)\n",
    "    return stats.pearsonr(values,aoa), scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_values(d_aoa, d_OSC1, d_OSC2, d_PSC1, d_PSC2, d_conc, d_valence, d_freq_subtlex, d_word2old, d_target_phon, d_morph, d_snd, d_iconicity, targets):\n",
    "    \"\"\"\n",
    "    :param d_*:       all dictionaries mapping words to values of variables of interest\n",
    "    :param targets:   list, sorted target words \n",
    "    :return:          list, values for target words \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for word in targets:\n",
    "        values.append([word, len(word), len(d_target_phon[word]), d_aoa[word], d_OSC1[word], d_OSC2[word], d_PSC1[word], d_PSC2[word], d_conc[word], d_valence[word], d_freq_subtlex[word], d_word2old[word], d_snd[word], d_iconicity[word], d_morph[word]])\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code\n",
    "\n",
    "## Load semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = SemanticSpace.from_csv('space.w2v.gz', prenorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the celex dictionary \n",
    "celex = json.load(open('celex_dict.json'))\n",
    "\n",
    "#load AoA ratings\n",
    "aoa_words, aoa_norms = read_aoa(\"AoA.xlsx\")\n",
    "aoa_ratings = pd.Series(aoa_norms[\"Rating.Mean\"].values,index=aoa_norms[\"Word\"]).to_dict()\n",
    "\n",
    "#load concreteness ratings\n",
    "conc = read_concreteness_norms(\"concreteness.txt\")\n",
    "conc_words = set(conc.keys())\n",
    "\n",
    "#load valence ratings\n",
    "valence = read_valence_norms(\"valence.csv\")\n",
    "val_words = set(valence.keys())\n",
    "\n",
    "#get list of words in the embedding space\n",
    "w2v_words = space.included_words()\n",
    "w2v_words_list = list(w2v_words)\n",
    "\n",
    "#load word frequencies\n",
    "freq_subtlex = read_frequency_subtlex(\"subtlex.csv\")\n",
    "freq_subtlex_words = set(freq_subtlex.keys())\n",
    "freq_subtlex_words_list = [x for x in freq_subtlex_words if str(x) != 'nan']\n",
    "\n",
    "#load old20 values\n",
    "word2old = read_old20(\"word2old.csv\")\n",
    "\n",
    "#get list of monomorphemic and polymorphemic words\n",
    "mono = list(read_mono(\"MorphoLEX_en.xlsx\"))\n",
    "poly = list(read_poly(\"MorphoLEX_en.xlsx\"))\n",
    "mono_inflected = list(read_mono_inflected(\"MorphoLEX_en.xlsx\"))\n",
    "\n",
    "#load iconicity ratings\n",
    "iconicity = read_iconicity(\"iconicity_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate multiple sets together for later filtering\n",
    "filter_words = list(itertools.chain(aoa_words,conc_words,val_words,mono,poly,mono_inflected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to create list of target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input argument 3, originally of type <class 'list'>, is coerced to set\n",
      "Input argument 5, originally of type <class 'dict'>, is coerced to set\n"
     ]
    }
   ],
   "source": [
    "#find shared words\n",
    "shared = shared_words(aoa_words,conc_words,val_words,w2v_words_list,freq_subtlex_words, word2old)\n",
    "\n",
    "#create morphological complexity variable for shared words\n",
    "word2morph = compute_morph_complexity(shared, mono, mono_inflected, poly)\n",
    "\n",
    "#only consider words that have a phonological encoding in CELEX\n",
    "target_words_phon = {k: v for k, v in get_celex_coverage(word2morph.keys(), celex)[0]}\n",
    "\n",
    "#create list of target words\n",
    "target_words = sorted(list(target_words_phon.keys()))\n",
    "\n",
    "#create dictionary mapping words to AoA ratings \n",
    "target_words_aoa = {k: v for k, v in aoa_ratings.items() if k in target_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2032/2032 [20:13<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#compute semantic neighbourhood density \n",
    "snd = compute_snd(space, target_words, filter_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf = open(\"target_neighbours_LD5_orth.pkl\",\"wb\")\\npickle.dump(target_neighbours_LD5_orth,f)\\nf.close()\\n         \\nf = open(\"target_neighbours_LD5_phon.pkl\",\"wb\")\\npickle.dump(target_neighbours_LD5_phon,f)\\nf.close()\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Â target-embedded neighbours (Marelli & Amenta)\n",
    "#target_neighbours_TE_orth = find_TE_neighbours(target_words, freq_subtlex_words_list)\n",
    "target_neighbours_TE_orth = pickle.load(open(\"target_neighbours_TE_orth.pkl\",\"rb\"))\n",
    "\n",
    "#target_neighbours_TE_phon = find_TE_neighbours(target_words_phon, freq_subtlex_words_list)\n",
    "target_neighbours_TE_phon = pickle.load(open(\"target_neighbours_TE_phon.pkl\",\"rb\"))\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"target_neighbours_TE_orth.pkl\",\"wb\")\n",
    "pickle.dump(target_neighbours_TE_orth,f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"target_neighbours_TE_phon.pkl\",\"wb\")\n",
    "pickle.dump(target_neighbours_TE_phon,f)\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "### edit-distance neighbours (Hendrix & Sun)\n",
    "\n",
    "#target_neighbours_LD5_orth = find_LD_neighbours(target_words, freq_subtlex_words_list, kn=5)\n",
    "target_neighbours_LD5_orth = pickle.load(open(\"target_neighbours_LD5_orth.pkl\",\"rb\"))\n",
    "\n",
    "\n",
    "#target_neighbours_LD5_phon = find_LD_neighbours(target_words_phon, freq_subtlex_words_list, kn=5)\n",
    "target_neighbours_LD5_phon = pickle.load(open(\"target_neighbours_LD5_phon.pkl\",\"rb\"))\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"target_neighbours_LD5_orth.pkl\",\"wb\")\n",
    "pickle.dump(target_neighbours_LD5_orth,f)\n",
    "f.close()\n",
    "         \n",
    "f = open(\"target_neighbours_LD5_phon.pkl\",\"wb\")\n",
    "pickle.dump(target_neighbours_LD5_phon,f)\n",
    "f.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf = open(\"PSC_hendrix.pkl\",\"wb\")\\npickle.dump(PSC_hendrix,f)\\nf.close()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### compute OSC values\n",
    "\n",
    "#OSC_marelli = compute_OPSC_ma(target_neighbours_TE_orth)\n",
    "OSC_marelli = pickle.load(open(\"OSC_marelli.pkl\", \"rb\"))\n",
    "\"\"\"\n",
    "f = open(\"OSC_marelli.pkl\",\"wb\")\n",
    "pickle.dump(OSC_marelli,f)\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "#OSC_hendrix = compute_OPSC_hs(target_neighbours_LD5_orth)\n",
    "OSC_hendrix = pickle.load(open(\"OSC_hendrix.pkl\", \"rb\"))\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"OSC_hendrix.pkl\",\"wb\")\n",
    "pickle.dump(OSC_hendrix,f)\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "### compute PSC values\n",
    "\n",
    "#PSC_marelli = compute_OPSC_ma(target_neighbours_TE_phon)\n",
    "PSC_marelli = pickle.load(open(\"PSC_marelli.pkl\", \"rb\"))\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"PSC_marelli.pkl\",\"wb\")\n",
    "pickle.dump(PSC_marelli,f)\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "#PSC_hendrix = compute_OPSC_hs(target_neighbours_LD5_phon)\n",
    "PSC_hendrix = pickle.load(open(\"PSC_hendrix.pkl\", \"rb\"))\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"PSC_hendrix.pkl\",\"wb\")\n",
    "pickle.dump(PSC_hendrix,f)\n",
    "f.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data analysis - check correlation with AoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OSC Marelli & Amenta\n",
    "print(check_correlation(target_words, OSC_marelli,target_words_aoa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OSC Hendrix & Sun\n",
    "print(check_correlation(target_words, OSC_hendrix,target_words_aoa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PSC Marelli & Amenta\n",
    "print(check_correlation(target_words, PSC_marelli,target_words_aoa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PSC Hendrix & Sun \n",
    "print(check_correlation(target_words, PSC_hendrix,target_words_aoa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final dataframe to export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = target_values(aoa_ratings, OSC_marelli, OSC_hendrix, PSC_marelli, PSC_hendrix, conc, valence, freq_subtlex, word2old, target_words_phon, word2morph, snd, target_words)\n",
    "\n",
    "final_df = pd.DataFrame(data=values, columns = [\"word\", \"word length\", \"nr_phon\", \"aoa\", \"OSC_m\", \"OSC_h\", \"PSC_m\", \"PSC_h\", \"conc\", \"val\", \"freq_subtlex\", \"old20\", \"snd\",\"morph\"])\n",
    "\n",
    "final_df.to_csv(\"thesis_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative OSC and PSC definitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSC & PSC 2 - target embedded & nr. of neighbours normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_OPSC2(tn_dict):\n",
    "    \"\"\"\n",
    "    :param tn_dict:    dictionary mapping target words to list of target-embedded neighbours \n",
    "    :return:           dictionary mapping target words to OPSC value\n",
    "    \"\"\"\n",
    "    opsc_dict = defaultdict(int)\n",
    "\n",
    "    for target in tqdm(tn_dict):\n",
    "        nom = 0\n",
    "        denom = len(tn_dict[target])\n",
    "        for n in tn_dict[target]:\n",
    "            try:\n",
    "                nom += space.pair_distance(target, n) \n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        opsc_dict[target] = nom/denom\n",
    "    \n",
    "    return opsc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSC & PSC 3 - LD neighbours & frequency normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_OPSC3(tn_dict):\n",
    "    \"\"\"\n",
    "    :param tn_dict:    dictionary mapping target keys to list of (neighbour, distance) tuples\n",
    "    :return:           dictionary mapping target words to OPSC value\n",
    "    \"\"\"\n",
    "    opsc_dict = defaultdict(int)\n",
    "\n",
    "    for target in tqdm(tn_dict):\n",
    "        nom = 0\n",
    "        denom = 0\n",
    "        for n, dis in tn_dict[target]:\n",
    "            denom += freq_subtlex[n]\n",
    "            try:\n",
    "                nom += space.pair_distance(target, n) * freq_subtlex[n] * 1/dis\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        opsc_dict[target] = nom/denom        \n",
    "    \n",
    "    return opsc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline Robustness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_word_vectors = space.word_vectors_matrix(space.included_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vectors = np.random.permutation(true_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
